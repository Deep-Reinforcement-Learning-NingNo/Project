 16%|█████████████████▉                                                                                              | 1597/10000 [01:04<04:53, 28.59it/s]
[Episode 100] Avg Reward: -7.68 | Policy Loss: -0.0069 | Value Loss: 7.1777
[INFO] Saved PPO policy checkpoint at ./saved_models_ppo_flappy\ppo_policy_ep100.pt
[Episode 200] Avg Reward: -7.42 | Policy Loss: -0.0069 | Value Loss: 1.3874
[INFO] Saved PPO policy checkpoint at ./saved_models_ppo_flappy\ppo_policy_ep200.pt
[Episode 300] Avg Reward: -7.58 | Policy Loss: -0.0069 | Value Loss: 0.9350
[INFO] Saved PPO policy checkpoint at ./saved_models_ppo_flappy\ppo_policy_ep300.pt
[Episode 400] Avg Reward: -7.67 | Policy Loss: -0.0069 | Value Loss: 0.6349
[INFO] Saved PPO policy checkpoint at ./saved_models_ppo_flappy\ppo_policy_ep400.pt
[Episode 500] Avg Reward: -7.53 | Policy Loss: -0.0069 | Value Loss: 0.6438
[INFO] Saved PPO policy checkpoint at ./saved_models_ppo_flappy\ppo_policy_ep500.pt
[Episode 600] Avg Reward: -7.67 | Policy Loss: -0.0069 | Value Loss: 0.6540
[INFO] Saved PPO policy checkpoint at ./saved_models_ppo_flappy\ppo_policy_ep600.pt
[Episode 700] Avg Reward: -7.37 | Policy Loss: -0.0069 | Value Loss: 0.6729
[INFO] Saved PPO policy checkpoint at ./saved_models_ppo_flappy\ppo_policy_ep700.pt
[Episode 800] Avg Reward: -7.47 | Policy Loss: -0.0069 | Value Loss: 0.7033
[INFO] Saved PPO policy checkpoint at ./saved_models_ppo_flappy\ppo_policy_ep800.pt
[Episode 900] Avg Reward: -7.57 | Policy Loss: -0.0069 | Value Loss: 0.5855
[INFO] Saved PPO policy checkpoint at ./saved_models_ppo_flappy\ppo_policy_ep900.pt
[Episode 1000] Avg Reward: -7.60 | Policy Loss: -0.0069 | Value Loss: 0.5836
[INFO] Saved PPO policy checkpoint at ./saved_models_ppo_flappy\ppo_policy_ep1000.pt
[Episode 1100] Avg Reward: -7.52 | Policy Loss: -0.0069 | Value Loss: 0.7464
[INFO] Saved PPO policy checkpoint at ./saved_models_ppo_flappy\ppo_policy_ep1100.pt
[Episode 1200] Avg Reward: -7.55 | Policy Loss: -0.0069 | Value Loss: 0.6078
[INFO] Saved PPO policy checkpoint at ./saved_models_ppo_flappy\ppo_policy_ep1200.pt
[Episode 1300] Avg Reward: -7.45 | Policy Loss: -0.0069 | Value Loss: 0.6439
[INFO] Saved PPO policy checkpoint at ./saved_models_ppo_flappy\ppo_policy_ep1300.pt
[Episode 1400] Avg Reward: -7.36 | Policy Loss: -0.0069 | Value Loss: 0.7061
[INFO] Saved PPO policy checkpoint at ./saved_models_ppo_flappy\ppo_policy_ep1400.pt
[Episode 1500] Avg Reward: -7.54 | Policy Loss: -0.0069 | Value Loss: 0.5820
[INFO] Saved PPO policy checkpoint at ./saved_models_ppo_flappy\ppo_policy_ep1500.pt
[Episode 1600] Avg Reward: -7.43 | Policy Loss: -0.0069 | Value Loss: 0.6345
[INFO] Saved PPO policy checkpoint at ./saved_models_ppo_flappy\ppo_policy_ep1600.pt
[Episode 1700] Avg Reward: -7.48 | Policy Loss: -0.0069 | Value Loss: 0.6987
[INFO] Saved PPO policy checkpoint at ./saved_models_ppo_flappy\ppo_policy_ep1700.pt
[Episode 1800] Avg Reward: -7.55 | Policy Loss: -0.0069 | Value Loss: 0.4817
[INFO] Saved PPO policy checkpoint at ./saved_models_ppo_flappy\ppo_policy_ep1800.pt
Traceback (most recent call last):
  File "D:\Fibo\term3_2\DRL\Project\Project\flappy_PPO_Train.py", line 83, in <module>
    episode_reward, policy_loss, value_loss = agent.learn(
                                              ^^^^^^^^^^^^
  File "D:\Fibo\term3_2\DRL\Project\Project\RL_Algorithm\Function_based\PPO.py", line 145, in learn
    policy_loss, value_loss = self.update_policy(states, actions, old_log_probs, returns)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Fibo\term3_2\DRL\Project\Project\RL_Algorithm\Function_based\PPO.py", line 123, in update_policy
    self.policy_optimizer.step()
  File "D:\Fibo\term3_2\DRL\Project\Project\.venv\Lib\site-packages\torch\optim\optimizer.py", line 485, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "D:\Fibo\term3_2\DRL\Project\Project\.venv\Lib\site-packages\torch\optim\optimizer.py", line 79, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Fibo\term3_2\DRL\Project\Project\.venv\Lib\site-packages\torch\optim\adam.py", line 246, in step
    adam(
  File "D:\Fibo\term3_2\DRL\Project\Project\.venv\Lib\site-packages\torch\optim\optimizer.py", line 147, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\Fibo\term3_2\DRL\Project\Project\.venv\Lib\site-packages\torch\optim\adam.py", line 933, in adam
    func(
  File "D:\Fibo\term3_2\DRL\Project\Project\.venv\Lib\site-packages\torch\optim\adam.py", line 525, in _single_tensor_adam
    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)
             ^^^^^^^^^^^^^^^^^
KeyboardInterrupt
